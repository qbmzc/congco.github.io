---
title: JVM｜MaxDirectMemorySize
date: 2021-10-12
categories:
  - Java
tags:
  - JVM
---

![bg](https://gitee.com/snowyan/image/raw/master/2021/202110131102827.png)

<!-- more -->

## 问题

运维的同事反映，生产环境的服务器内存超过90%的占用，jvm相关配置如下

## 配置

```bash
-server -Xms12g -Xmx12g -XX:+UseG1GC -XX:MaxGCPauseMillis=200  -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Dspring.profiles.active=PROD -XX:CompressedClassSpaceSize=1g 
```

物理机内存为16G,该服务为文件服务，文件上传中有使用到直接内存（堆外内存）。

```bash
# 设置堆外内存大小
-XX:MaxDirectMemorySize=2g
```

## meminfo内容分析

```bash
cat /proc/meminfo
```

实际内存大小为15.5 堆内存设置为12 堆外为2 其他320M 所以占用92%为正常情况

## meminfo内容分析

| 属性               | 大小(k)     | 说明                                                         | 扩展说明                                                     |
| ------------------ | ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| MemTotal:          | 16266400    | 可供linux内核分配的内存总量。                                | 比物理内存总量少一点，因为主板/固件会保留一部分内存、linux内核自己也会占用一部分内存。 |
| MemFree:           | 251600      | 表示系统尚未分配的内存。                                     |                                                              |
| MemAvailable:      | 3468952     | 当前可用内存。                                               | MemFree只是尚未分配的内存，并不是所有可用的内存。有些已经分配掉的内存是可以回收再分配的。比如cache/buffer、slab都有一部分是可以回收的，这部分可回收的内存加上MemFree才是系统可用的内存，即MemAvailable。同时要注意，MemAvailable是内核使用特定的算法估算出来的，并不精确。 |
| Buffers:           | 93764       | 块设备(block device)所占用的特殊file-backed pages，包括：直接读写块设备，以及文件系统元数据(metadata)比如superblock使用的缓存页。 | Buffers内存页同时也在LRU list中，被统计在Active(file)或Inactive(file)之中。 |
| Cached:            | 3216600     | 所有file-backed pages                                        | 用户进程的内存页分为两种：file-backed pages（与文件对应的内存页），和anonymous pages（匿名页），比如进程的代码、映射的文件都是file-backed，而进程的堆、栈都是不与文件相对应的、就属于匿名页。file-backed pages在内存不足的时候可以直接写回对应的硬盘文件里，称为page-out，不需要用到交换区(swap)；而anonymous pages在内存不足时就只能写到硬盘上的交换区(swap)里，称为swap-out。 |
| SwapCached:        | 0           | SwapCached包含的是被确定要swap-out，但是尚未写入交换区的匿名内存页。 | SwapCached内存页会同时被统计在LRU或AnonPages或Shmem中，它本身并不占用额外的内存。 |
| Active:            | 12247224    | active包含active anon和active file                           | LRU是一种内存页回收算法，Least Recently Used,最近最少使用。LRU认为，在最近时间段内被访问的数据在以后被再次访问的概率，要高于最近一直没被访问的页面。于是近期未被访问到的页面就成为了页面回收的第一选择。Linux kernel会记录每个页面的近期访问次数，然后设计了两种LRU list: active list 和 inactive list, 刚访问过的页面放进active list，长时间未访问过的页面放进inactive list，回收内存页时，直接找inactive list即可。另外，内核线程kswapd会周期性地把active list中符合条件的页面移到inactive list中。 |
| Inactive:          | 1213960     | inactive包含inactive anon和inactive file                     |                                                              |
| Active(anon):      | 10450776    | 活跃匿名页，anonymous pages（匿名页）。                      |                                                              |
| Inactive(anon):    | 1755780     | 非活跃匿名页                                                 |                                                              |
| Active(file):      | 1796448     | 活跃文件内存页                                               |                                                              |
| Inactive(file):    | 1512640     | 非活跃文件内存页                                             |                                                              |
| Unevictable:       | 0           | 因为种种原因无法回收(page-out)或者交换到swap(swap-out)的内存页 | Unevictable LRU list上是不能pageout/swapout的内存页，包括VM_LOCKED的内存页、SHM_LOCK的共享内存页（同时被统计在Mlocked中）、和ramfs。在unevictable list出现之前，这些内存页都在Active/Inactive lists上，vmscan每次都要扫过它们，但是又不能把它们pageout/swapout，这在大内存的系统上会严重影响性能，unevictable list的初衷就是避免这种情况的发生。 |
| Mlocked:           | 0           | 被系统调用"mlock()"锁定到内存中的页面。Mlocked页面是不可收回的。 | 被锁定的内存因为不能pageout/swapout，会从Active/Inactive LRU list移到Unevictable LRU list上。Mlocked与以下统计项重叠：LRU Unevictable，AnonPages，Shmem，Mapped等。 |
| SwapTotal:         | 8388604     | swap空间总计                                                 |                                                              |
| SwapFree:          | 8388604     | 当前剩余swap                                                 |                                                              |
| Dirty:             | 736         | 需要写入磁盘的内存页的大小                                   | Dirty并不包括系统中全部的dirty pages，需要再加上另外两项：NFS_Unstable 和 Writeback，NFS_Unstable是发给NFS server但尚未写入硬盘的缓存页，Writeback是正准备回写硬盘的缓存页。 |
| Writeback:         | 0           | 正在被写回的内存页的大小                                     |                                                              |
| AnonPages:         | 12205280    | Anonymous pages(匿名页)数量 + AnonHugePages(透明大页)数量    | 进程所占的内存页分为anonymous pages和file-backed pages，理论上，`所有进程的PSS之和 = Mapped + AnonPages`。PSS是Proportional Set Size，每个进程实际使用的物理内存（比例分配共享库占用的内存），可以在`/proc/[1-9]*/smaps`中查看。 |
| Mapped:            | 73716       | 正被用户进程关联的file-backed pages                          | Cached包含了所有file-backed pages，其中有些文件当前不在使用，但Cached仍然可能保留着它们的file-backed pages；而另一些文件正被用户进程关联，比如shared libraries、可执行程序的文件、mmap的文件等，这些文件的缓存页就称为mapped。 |
| Shmem:             | 1276        | Shmem统计的内容包括：1.shared memory;2.tmpfs和devtmpfs。所有tmpfs类型的文件系统占用的空间都计入共享内存，devtmpfs是/dev文件系统的类型，/dev/下所有的文件占用的空间也属于共享内存。可以用ls和du命令查看。如果文件在没有关闭的情况下被删除，空间仍然不会释放，shmem不会减小，可以用 `lsof -a +L1 /<mount_point>` 命令列出这样的文件。 | shared memory被视为基于tmpfs文件系统的内存页，既然基于文件系统，就不算匿名页，所以不被计入/proc/meminfo中的AnonPages，而是被统计进了：`Cached`或`Mapped`(当shmem被attached时候)。然而它们背后并不存在真正的硬盘文件，一旦内存不足的时候，它们是需要交换区才能swap-out的，所以在LRU lists里，它们被放在`Inactive(anon)` 或 `Active(anon)`或 `unevictable` （如果被locked的话）里。注意：/proc/meminfo中的 Shmem 统计的是已经分配的大小，而不是创建时申请的大小。 |
| Slab:              | 276736      | 通过slab分配的内存，Slab=SReclaimable+SUnreclaim             | slab是linux内核的一种内存分配器。linux内核的动态内存分配有以下几种方式：1.`alloc_pages/__get_free_page`:以页为单位分配。2.`vmalloc`:以字节为单位分配虚拟地址连续的内存块。3.`slab`:对小对象进行分配，不用为每个小对象分配一个页，节省了空间；内核中一些小对象创建析构很频繁，Slab对这些小对象做缓存，可以重复利用一些相同的对象，减少内存分配次数。4.`kmalloc`：以slab为基础，以字节为单位分配物理地址连续的内存块。 |
| SReclaimable:      | 246604      | slab中可回收的部分。                                         |                                                              |
| SUnreclaim:        | 30132       | slab中不可回收的部分。                                       |                                                              |
| KernelStack:       | 8096        | 给用户线程分配的内核栈消耗的内存页                           | 每一个用户线程都会分配一个kernel stack（内核栈），内核栈虽然属于线程，但用户态的代码不能访问，只有通过系统调用(syscall)、自陷(trap)或异常(exception)进入内核态的时候才会用到，也就是说内核栈是给kernel code使用的。在x86系统上Linux的内核栈大小是固定的8K或16K。Kernel stack（内核栈）是常驻内存的，既不包括在LRU lists里，也不包括在进程的RSS/PSS内存里。RSS是Resident Set Size 实际使用物理内存（包含共享库占用的内存），可以在`/proc/[1-9]*/smaps`中查看。 |
| PageTables:        | 33060       | Page Table的消耗的内存页                                     | Page Table的用途是翻译虚拟地址和物理地址，它是会动态变化的，要从MemTotal中消耗内存。 |
| NFS_Unstable:      | 0           | 发给NFS server但尚未写入硬盘的缓存页                         |                                                              |
| Bounce:            | 0           | bounce buffering消耗的内存页                                 | 有些老设备只能访问低端内存，比如16M以下的内存，当应用程序发出一个I/O 请求，DMA的目的地址却是高端内存时（比如在16M以上），内核将在低端内存中分配一个临时buffer作为跳转，把位于高端内存的缓存数据复制到此处。 |
| WritebackTmp:      | 0           | 正准备回写硬盘的缓存页                                       |                                                              |
| CommitLimit:       | 16521804    | overcommit阈值，CommitLimit = (Physical RAM * vm.overcommit_ratio / 100) + Swap | Linux是允许memory overcommit的，即承诺给进程的内存大小超过了实际可用的内存。commit(或overcommit)针对的是内存申请，内存申请不等于内存分配，内存只在实际用到的时候才分配。但可以申请的内存有个上限阈值，即CommitLimit，超出以后就不能再申请了。 |
| Committed_AS:      | 15198636    | 所有进程已经申请的内存总大小                                 |                                                              |
| VmallocTotal:      | 34359738367 | 可分配的虚拟内存总计                                         |                                                              |
| VmallocUsed:       | 38120       | 已通过vmalloc分配的内存，不止包括了分配的物理内存，还统计了VM_IOREMAP、VM_MAP等操作的值 | VM_IOREMAP是把IO地址映射到内核空间、并未消耗物理内存         |
| VmallocChunk:      | 34359608232 | 通过vmalloc可分配的虚拟地址连续的最大内存                    |                                                              |
| HardwareCorrupted: | 0           | 因为内存的硬件故障而删除的内存页                             |                                                              |
| AnonHugePages:     | 8065024     | AnonHugePages统计的是Transparent HugePages (THP)，THP与Hugepages不是一回事，区别很大。Hugepages在/proc/meminfo中是被独立统计的，与其它统计项不重叠，既不计入进程的RSS/PSS中，又不计入LRU Active/Inactive，也不会计入cache/buffer。如果进程使用了Hugepages，它的RSS/PSS不会增加。而AnonHugePages完全不同，它与/proc/meminfo的其他统计项有重叠，首先它被包含在AnonPages之中，而且在/proc/<pid>/smaps中也有单个进程的统计，与进程的RSS/PSS是有重叠的，如果用户进程用到了THP，进程的RSS/PSS也会相应增加，这与Hugepages是不同的。 | Transparent Huge Pages 缩写 THP ，这个是 RHEL 6 开始引入的一个功能，在 Linux6 上透明大页是默认启用的。由于 Huge pages 很难手动管理，而且通常需要对代码进行重大的更改才能有效的使用，因此 RHEL 6 开始引入了 Transparent Huge Pages （ THP ）， THP 是一个抽象层，能够自动创建、管理和使用传统大页。THP 为系统管理员和开发人员减少了很多使用传统大页的复杂性 , 因为 THP 的目标是改进性能 , 因此其它开发人员 ( 来自社区和红帽 ) 已在各种系统、配置、应用程序和负载中对 THP 进行了测试和优化。这样可让 THP 的默认设置改进大多数系统配置性能。但是 , 不建议对数据库工作负载使用 THP 。这两者最大的区别在于 : 标准大页管理是预分配的方式，而透明大页管理则是动态分配的方式。 |
| HugePages_Total:   | 0           | 预分配的可使用的标准大页池的大小。HugePages在内核中独立管理，只要一经定义，无论是否被使用，都不再属于free memory。 | Huge pages(标准大页) 是从 Linux Kernel 2.6 后被引入的，目的是通过使用大页内存来取代传统的 4kb 内存页面， 以适应越来越大的系统内存，让操作系统可以支持现代硬件架构的大页面容量功能。 |
| HugePages_Free:    | 0           | 标准大页池中尚未分配的标准大页                               |                                                              |
| HugePages_Rsvd:    | 0           | 用户程序预申请的标准大页，尚未真的分配走                     |                                                              |
| HugePages_Surp:    | 0           | 标准大页池的盈余                                             |                                                              |
| Hugepagesize:      | 2048        | 标准大页大小，这里是2M                                       |                                                              |
| DirectMap4k:       | 210816      | 映射为4kB的内存数量                                          | DirectMap所统计的不是关于内存的使用，而是一个反映TLB效率的指标。TLB(Translation Lookaside Buffer)是位于CPU上的缓存，用于将内存的虚拟地址翻译成物理地址，由于TLB的大小有限，不能缓存的地址就需要访问内存里的page table来进行翻译，速度慢很多。为了尽可能地将地址放进TLB缓存，新的CPU硬件支持比4k更大的页面从而达到减少地址数量的目的， 比如2MB，4MB，甚至1GB的内存页，视不同的硬件而定。所以DirectMap其实是一个反映TLB效率的指标。 |
| DirectMap2M:       | 9226240     | 映射为2MB的内存数量                                          |                                                              |
| DirectMap1G:       | 9437184     | 映射为1GB的内存数量                                          |                                                              |

## 参考资料

- [JVM源码分析之堆外内存完全解读](http://lovestblog.cn/blog/2015/05/12/direct-buffer/)
- [堆外内存 之 DirectByteBuffer 详解](https://www.jianshu.com/p/007052ee3773)
- [[linux内存占用分析之meminfo](https://segmentfault.com/a/1190000022518282)](https://segmentfault.com/a/1190000022518282)
