定时任务技术调研


---

## 背景

目前job-service因为使用的quartz调度，存在以下问题：

1、每次只能单副本运行，无法发挥集群分布式的优势，后期定时任务多起来以后单副本压力会很大

2、每隔十分钟需要从数据库中重新捞一遍定时任务数据加载到quartz中，在重新加载过程中有几率导致本次某个任务丢失不执行

- 需要考虑引入新的分布式调度组件（PowerJob、xxl-job等）

NICE TO HAVE（重要）：

1、可以提高稳定性

2、可以提供服务性能

## Quartz(集群)

- Scheduler:调度器
- Trigger:触发器
- Job:任务

| 类型          | 优点                                                         | 缺点                                                         |
| :------------ | :----------------------------------------------------------- | :----------------------------------------------------------- |
| RAMJobStore   | 不要外部数据库，配置容易，运行速度快                         | 因为调度程序信息是存储在被分配给 JVM 的内存里面，所以，当应用程序停止运行时，所有调度信息将被丢失。另外因为存储到JVM内存里面，所以可以存储多少个 Job 和 Trigger 将会受到限制 |
| JDBC 作业存储 | 支持集群，因为所有的任务信息都会保存到数据库中，可以控制事物，还有就是如果应用服务器关闭或者重启，任务信息都不会丢失，并且可以恢复因服务器关闭或者重启而导致执行失败的任务 | 运行速度的快慢取决与连接数据库的快慢                         |

### 引入依赖

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	<parent>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-parent</artifactId>
		<version>3.2.2</version>
		<relativePath/> <!-- lookup parent from repository -->
	</parent>
	<groupId>com.cong</groupId>
	<artifactId>job-demo1</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<name>job-demo1</name>
	<description>Demo project for Spring Boot</description>
	<properties>
		<java.version>21</java.version>
	</properties>
	<dependencies>

		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-actuator</artifactId>
		</dependency>

		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-data-jdbc</artifactId>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-quartz</artifactId>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-web</artifactId>
		</dependency>

		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-devtools</artifactId>
			<scope>runtime</scope>
			<optional>true</optional>
		</dependency>
		<dependency>
			<groupId>com.mysql</groupId>
			<artifactId>mysql-connector-j</artifactId>
			<scope>runtime</scope>
		</dependency>
		<dependency>
			<groupId>org.projectlombok</groupId>
			<artifactId>lombok</artifactId>
			<optional>true</optional>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-test</artifactId>
			<scope>test</scope>
		</dependency>
	</dependencies>

	<build>
		<plugins>
			<plugin>
				<groupId>org.springframework.boot</groupId>
				<artifactId>spring-boot-maven-plugin</artifactId>
				<configuration>
					<excludes>
						<exclude>
							<groupId>org.projectlombok</groupId>
							<artifactId>lombok</artifactId>
						</exclude>
					</excludes>
				</configuration>
			</plugin>
		</plugins>
	</build>

</project>

```

### 示例Job

- Job1

```java
package com.cong.jobdemo1.config.job;

import org.quartz.DisallowConcurrentExecution;
import org.quartz.JobExecutionContext;
import org.quartz.JobExecutionException;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.scheduling.quartz.QuartzJobBean;

import com.cong.jobdemo1.service.DemoService;

import lombok.extern.slf4j.Slf4j;

/**
* 添加了 Quartz 的 @DisallowConcurrentExecution 注解，保证相同 JobDetail 在多个 JVM 进程中，有且仅有一个节点在执行。
*/
@Slf4j
@DisallowConcurrentExecution
public class DemoJob01 extends QuartzJobBean{

    @Autowired
    private DemoService demoService;

    @Override
    protected void executeInternal(JobExecutionContext context) throws JobExecutionException {
      //自定义的定时任务逻辑
        log.info("[executeInternal][我开始执行了, demoService 为 ({})]", demoService);
    }
    
}

```

- job2

```java
package com.cong.jobdemo1.config.job;

import org.quartz.DisallowConcurrentExecution;
import org.quartz.JobExecutionContext;
import org.quartz.JobExecutionException;
import org.springframework.scheduling.quartz.QuartzJobBean;

import lombok.extern.slf4j.Slf4j;

@Slf4j
@DisallowConcurrentExecution
public class DemoJob02 extends QuartzJobBean{


    @Override
    protected void executeInternal(JobExecutionContext context) throws JobExecutionException {
        log.info("[executeInternal][我开始执行了]");
    }
    
}

```

`DisallowConcurrentExecution` 保证在多个JVM中有且仅有一个节点在执行，是以 _**JobDetail**_ 为依据的

而 JobDetail 的**唯一标识**是 JobKey ，使用 `name` + `group` 两个属性。一般情况下，我们只需要设置 `name` 即可，而 Quartz 会默认 `group = DEFAULT` 。

### 配置文件

```yaml
server:
  port: 13002
  shutdown: graceful

spring:
  datasource:
    user:
      url: jdbc:mysql://192.168.70.58:3307/job?useSSL=false&allowPublicKeyRetrieval=true&useUnicode=true&characterEncoding=UTF-8
      driver-class-name: com.mysql.cj.jdbc.Driver
      username: root
      password: 123456
    quartz:
      url: jdbc:mysql://192.168.70.58:3307/quartz?useSSL=false&allowPublicKeyRetrieval=true&useUnicode=true&characterEncoding=UTF-8
      driver-class-name: com.mysql.cj.jdbc.Driver
      username: root
      password: 123456

  # Quartz 的配置，对应 QuartzProperties 配置类
  quartz:
    scheduler-name: clusteredScheduler # Scheduler 名字。默认为 schedulerName，相同 Scheduler 名字的节点，形成一个 Quartz 集群。
    job-store-type: jdbc # Job 存储器类型。默认为 memory 表示内存，可选 jdbc 使用数据库。
    auto-startup: true # Quartz 是否自动启动
    startup-delay: 0 # 延迟 N 秒启动
    wait-for-jobs-to-complete-on-shutdown: true # 应用关闭时，是否等待定时任务执行完成。默认为 false ，建议设置为 true
    overwrite-existing-jobs: false # 是否覆盖已有 Job 的配置
    properties: # 添加 Quartz Scheduler 附加属性，更多可以看 http://www.quartz-scheduler.org/documentation/2.4.0-SNAPSHOT/configuration.html 文档
      org:
        quartz:
          scheduler:
            # 调度器实例名称
            instanceName: QuartzScheduler
            # 分布式节点ID自动生成
            instanceId: AUTO
          # JobStore 相关配置
          jobStore:
            # 数据源名称
            dataSource: quartzDataSource # 使用的数据源

            class: org.springframework.scheduling.quartz.LocalDataSourceJobStore
            driverDelegateClass: org.quartz.impl.jdbcjobstore.StdJDBCDelegate
            tablePrefix: QRTZ_ # Quartz 表前缀
            isClustered: true # 是集群模式
            clusterCheckinInterval: 1000
            useProperties: false
          # 线程池相关配置
          threadPool:
            threadCount: 25 # 线程池大小。默认为 10 。
            threadPriority: 5 # 线程优先级
            class: org.quartz.simpl.SimpleThreadPool # 线程池类型

    jdbc: # 使用 JDBC 的 JobStore 的时候，JDBC 的配置
      initialize-schema: always # 是否自动使用 SQL 初始化 Quartz 表结构。设置成 never ，需要手动创建表结构。
```

### Quartz表结构

![image-20240201152631029](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/202402011526256.png)

每个表都有一个 SCHED_NAME 字段，Quartz Scheduler 名字。这样，实现每个 Quartz 集群，数据层面的拆分

### DataSourceConfiguration

```java
package com.cong.jobdemo1.config;

import javax.sql.DataSource;

import org.springframework.boot.autoconfigure.jdbc.DataSourceProperties;
import org.springframework.boot.autoconfigure.quartz.QuartzDataSource;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.util.StringUtils;

import com.zaxxer.hikari.HikariDataSource;

@Configuration
public class DataSourceConfiguration {

    /**
     * 创建 user 数据源的配置对象
     */
    @Primary
    @Bean("userDataSourceProperties")
    @ConfigurationProperties(prefix = "spring.datasource.user")
    public DataSourceProperties userDataSourceProperties() {
        return new DataSourceProperties();
    }

    /**
     * 创建 user 数据源
     */
    @Primary
    @Bean(name = "userDataSource")
    @ConfigurationProperties(prefix = "spring.datasource.user.hikari") // 读取 spring.datasource.user 配置到 HikariDataSource
                                                                       // 对象
    public DataSource userDataSource() {
        // 获得 DataSourceProperties 对象
        DataSourceProperties properties = this.userDataSourceProperties();
        // 创建 HikariDataSource 对象
        return createHikariDataSource(properties);
    }

    /**
     * 创建 quartz 数据源的配置对象
     */
    @Bean(name = "quartzDataSourceProperties")
    @ConfigurationProperties(prefix = "spring.datasource.quartz") // 读取 spring.datasource.quartz 配置到
                                                                  // DataSourceProperties 对象
    public DataSourceProperties quartzDataSourceProperties() {
        return new DataSourceProperties();
    }

    /**
     * 创建 quartz 数据源
     * @QuartzDataSource 注解为我们配置和初始化 Quartz 数据库
     */
    @Bean(name = "quartzDataSource")
    @ConfigurationProperties(prefix = "spring.datasource.quartz.hikari")
    @QuartzDataSource
    public DataSource quartzDataSource() {
        // 获得 DataSourceProperties 对象
        DataSourceProperties properties = this.quartzDataSourceProperties();
        // 创建 HikariDataSource 对象
        return createHikariDataSource(properties);
    }

    private static HikariDataSource createHikariDataSource(DataSourceProperties properties) {
        // 创建 HikariDataSource 对象
        HikariDataSource dataSource = properties.initializeDataSourceBuilder().type(HikariDataSource.class).build();
        // 设置线程池名
        if (StringUtils.hasText(properties.getName())) {
            dataSource.setPoolName(properties.getName());
        }
        return dataSource;
    }

}

```

### 定时任务配置

- Bean自动设置

```java
package com.cong.jobdemo1.config;

import org.quartz.CronScheduleBuilder;
import org.quartz.JobBuilder;
import org.quartz.JobDetail;
import org.quartz.SimpleScheduleBuilder;
import org.quartz.Trigger;
import org.quartz.TriggerBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import com.cong.jobdemo1.config.job.DemoJob01;
import com.cong.jobdemo1.config.job.DemoJob02;

@Configuration
public class ScheduleConfiguration {

    public static class DemoJob01Configuration {

        @Bean
        public JobDetail demoJob01() {
            return JobBuilder.newJob(DemoJob01.class)
                    .withIdentity("demoJob01") // 名字为 demoJob01
                    .storeDurably() // 没有 Trigger 关联的时候任务是否被保留。因为创建 JobDetail 时，还没 Trigger 指向它，所以需要设置为 true ，表示保留。
                    .build();
        }

        @Bean
        public Trigger demoJob01Trigger() {
            // 简单的调度计划的构造器
            SimpleScheduleBuilder scheduleBuilder = SimpleScheduleBuilder.simpleSchedule()
                    .withIntervalInSeconds(5) // 频率。
                    .repeatForever(); // 次数。
            // Trigger 构造器
            return TriggerBuilder.newTrigger()
                    .forJob(demoJob01()) // 对应 Job 为 demoJob01
                    .withIdentity("demoJob01Trigger") // 名字为 demoJob01Trigger
                    .withSchedule(scheduleBuilder) // 对应 Schedule 为 scheduleBuilder
                    .build();
        }

    }

    public static class DemoJob02Configuration {

        @Bean
        public JobDetail demoJob02() {
            return JobBuilder.newJob(DemoJob02.class)
                    .withIdentity("demoJob02") // 名字为 demoJob02
                    .storeDurably() // 没有 Trigger 关联的时候任务是否被保留。因为创建 JobDetail 时，还没 Trigger 指向它，所以需要设置为 true ，表示保留。
                    .build();
        }

        @Bean
        public Trigger demoJob02Trigger() {
            // 简单的调度计划的构造器
            CronScheduleBuilder scheduleBuilder = CronScheduleBuilder.cronSchedule("0/10 * * * * ? *");
            // Trigger 构造器
            return TriggerBuilder.newTrigger()
                    .forJob(demoJob02()) // 对应 Job 为 demoJob02
                    .withIdentity("demoJob02Trigger") // 名字为 demoJob02Trigger
                    .withSchedule(scheduleBuilder) // 对应 Schedule 为 scheduleBuilder
                    .build();
        }

    }

}
```

在 Quartz 调度器启动的时候，会根据该配置，自动调用如下方法：

​	 `Scheduler#addJob(JobDetail jobDetail, boolean replace)` 方法，将 JobDetail 持久化到数据库。

   	`Scheduler#scheduleJob(Trigger trigger)` 方法，将 Trigger 持久化到数据库。

- Scheduler手动设置

```java
package com.cong.jobdemo1.controller;

import java.util.UUID;

import org.quartz.JobBuilder;
import org.quartz.JobDetail;
import org.quartz.Scheduler;
import org.quartz.SimpleScheduleBuilder;
import org.quartz.SimpleTrigger;
import org.quartz.TriggerBuilder;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import com.cong.jobdemo1.config.job.DemoJob03;

import lombok.SneakyThrows;
import lombok.extern.slf4j.Slf4j;

@Slf4j
@RestController
@RequestMapping("task")
public class TaskController {

    @Autowired
    private Scheduler scheduler;

    @SneakyThrows
    @GetMapping("add")
    public void getMethodName() {
        log.info("添加❤️的任务");
        JobDetail jobDetail = JobBuilder.newJob(DemoJob03.class)
                .withIdentity(UUID.randomUUID().toString())
                .storeDurably()
                .build();

        SimpleScheduleBuilder repeatForever = SimpleScheduleBuilder.simpleSchedule()
                .withIntervalInSeconds(10)
                .repeatForever();

        SimpleTrigger simpleTrigger = TriggerBuilder.newTrigger().forJob(jobDetail)
                .withIdentity(UUID.randomUUID().toString())
                .withSchedule(repeatForever)
                .build();

        /*
         * 如果想要覆盖数据库中的 Quartz 定时任务的配置，
         * 可以调用 Scheduler#scheduleJob(JobDetail jobDetail, Set<? extends Trigger>
         * triggersForJob, boolean replace) 方法，
         * 传入 replace = true 进行覆盖配置。
         */
        scheduler.scheduleJob(jobDetail, simpleTrigger);
        if (!scheduler.isShutdown()) {
            scheduler.start();
        }
    }

}

```

### 集群方式运行

![](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403041014543.webp)

#### 集群任务调度的原理

在集群模式下，Quartz通过数据库行锁来确保同一个任务不会被多个节点同时执行。当一个调度器实例尝试执行一个任务时，它会先在数据库中对该任务加锁。如果加锁成功，该实例就会执行任务；否则，意味着有其他实例已经在执行该任务，当前实例就会放弃执行。

BTW，分布式部署时需要保证各个节点的系统时间一致。

Quartz数据库核心表如下：

| Table Name               | Description                                                |
| :----------------------- | :--------------------------------------------------------- |
| QRTZ_CALENDARS           | 存储Quartz的Calendar信息                                   |
| QRTZ_CRON_TRIGGERS       | 存储CronTrigger，包括Cron表达式和时区信息                  |
| QRTZ_FIRED_TRIGGERS      | 存储与已触发的Trigger相关的状态信息，以及相联Job的执行信息 |
| QRTZ_PAUSED_TRIGGER_GRPS | 存储已暂停的Trigger组的信息                                |
| QRTZ_SCHEDULER_STATE     | 存储少量的有关Scheduler的状态信息，和别的Scheduler实例     |
| **QRTZ_LOCKS**           | **存储程序的悲观锁的信息**                                 |
| QRTZ_JOB_DETAILS         | 存储每一个已配置的Job的详细信息                            |
| QRTZ_JOB_LISTENERS       | 存储有关已配置的JobListener的信息                          |
| QRTZ_SIMPLE_TRIGGERS     | 存储简单的Trigger，包括重复次数、间隔、以及已触的次数      |
| QRTZ_BLOG_TRIGGERS       | Trigger作为Blob类型存储                                    |
| QRTZ_TRIGGER_LISTENERS   | 存储已配置的TriggerListener的信息                          |
| QRTZ_TRIGGERS            | 存储已配置的Trigger的信息                                  |

一个scheduler实例在集群模式下首先获取{0}LOCKS表中的行锁；

向Mysql获取杭锁的语句：



```sql
select * from {0}LOCKS where sched_name = ? and lock_name = ? for update
```

{0}会替换为配置文件默认配置的**`QRTZ_`**。sched_name为应用集群的实例名，lock_name就是行级锁名。Quartz主要由两个行级锁。

| lock_name      | desc         |
| :------------- | :----------- |
| STATE_ACCESS   | 状态访问锁   |
| TRIGGER_ACCESS | 触发器访问锁 |

Quartz集群争用触发器行锁，锁被占用只能等待，获取触发器行锁之后，先获取需要等待触发的其他触发器信息。数据库更新触发器状态信息，及时是否触发器行锁，供其他调度实例获取，然后在进行触发器任务调度操作，对数据库操作就要先获取行锁。

![Quartz集群基于锁的同步方案](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403041049320.png)

#### 同时执行的最大任务数

在Quartz任务调度框架中，并没有直接定义每个执行器（Executor）的容量上限这一概念。Quartz主要关注的是任务调度和管理，而非线程池大小或并发执行任务的数量这样的执行细节。



不过，如果指的是在Quartz中执行作业时的并发处理能力，那么可以通过配置调度器（Scheduler）使用的线程池来间接控制并发执行任务的数量。Quartz内部可以配置ThreadPool来管理执行作业的线程资源，默认实现是`org.quartz.simpl.SimpleThreadPool`。



在`SimpleThreadPool`中，你可以设置如下的参数来控制线程池的行为：



1. `threadCount`：线程池中的线程数量，这可视为“容量上限”之一。
2. `threadPriority`：线程的优先级。
3. `makeThreadsDaemons`：是否将线程设为守护线程。
4. `threadKeepAliveTime`：空闲线程等待新任务的时间，在此时间过后若无新任务则会终止线程。



配置示例：



\#java

```
Properties props = new Properties();
props.setProperty("org.quartz.threadPool.class", "org.quartz.simpl.SimpleThreadPool");
props.setProperty("org.quartz.threadPool.threadCount", "10");  // 设置线程池容量上限为10
schedulerFactoryBean.setSchedulerProperties(props);
```



通过调整这些参数，可以控制调度器能够同时执行的最大任务数，但这并不是对单个执行器的容量上限进行限制，而是整个调度器层面的并发控制。



#### Quartz处理耗时较长的任务

会出现`Handling 2 trigger(s) that missed their scheduled fire-time.`任务会丢失，需要手动设置处理。

Quartz Scheduler 可能会面临一些挑战和问题，具体取决于任务的性质、Quartz Scheduler 的配置以及环境条件等因素。以下是一些可能出现的问题和解决方法：

1. **任务执行超时：** 如果任务的执行时间超过了预期，Quartz Scheduler 可能会超出预定的调度间隔，导致调度的不稳定性。可以通过调整任务的执行时间、调度间隔或者使用并行执行来解决这个问题。
2. **资源竞争：** 长时间运行的任务可能会导致资源（如线程、内存）的竞争和耗尽，影响其他任务的执行。可以通过限制任务的并发数量、合理分配资源或者使用异步执行等方式来解决。
3. **数据库连接问题：** 如果任务涉及到数据库操作，长时间的任务可能会导致数据库连接的泄漏或者超时，影响其他任务的执行。可以通过使用连接池、合理管理数据库连接、优化数据库操作等方式来解决。
4. **任务重复执行：** 如果任务在执行期间发生异常或者执行时间超过了预期，Quartz Scheduler 可能会重新调度该任务，导致任务的重复执行。可以通过合理捕获异常、设置任务的状态、使用分布式锁等方式来避免任务的重复执行。
5. **任务状态管理：** 长时间运行的任务可能需要额外的状态管理，以便在执行期间进行监控、中断或者重新调度。可以通过设置任务的状态、使用监控工具、合理设计任务的执行逻辑等方式来管理任务的状态。



### 集群故障转移

每个服务器会定时（org.quartz.jobStore.clusterCheckinInterval这个时间）更新SCHEDULER_STATE表中的LAST_CHECK_TIME（**将服务器的当前时刻更新为最后更新时刻**）字段，遍历集群各兄弟节点的实例状态，检测集群各个兄弟节点的健康状态。

![image-20240304102313076](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403041023141.png)

Quartz使用了一个随机的负载均衡算法，Job以随机的方式由不同的实例执行。

#### 处理集群环境下的任务冲突和容错

为了有效地在集群环境下管理任务调度，需要考虑到任务冲突和容错的问题。任务冲突通常发生在两个或两个以上的调度器实例尝试同时执行同一个任务的情况。如前所述，Quartz通过数据库锁机制来避免这种情况的发生。

容错机制是指当一个调度器实例失败时，集群中的其他实例能够接管未完成的任务。Quartz通过持续检查数据库中的锁和任务状态来实现这一点。如果一个实例在执行任务时失败，数据库中的锁将被释放，其他实例就可以接管并执行该任务。

通过部署Quartz到集群环境，能够提高任务调度的可靠性和可用性，确保即使在部分节点出现故障的情况下，任务调度也能正常进行。这对于构建高可用的大规模应用至关重要。

![image-20240301181312744](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403011813806.png)



启动多个实例，本地启动 注意端口不能重复

![image-20240201161729611](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/202402011617651.png)

![image-20240201163159666](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/202402011631693.png)

![image-20240201163324093](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/202402011633120.png)

---

## [XXL-JOB](https://github.com/xuxueli/xxl-job/)

> XXL-JOB 是一个轻量级分布式任务调度平台，其核心设计目标是开发迅速、学习简单、轻量级、易扩展。

### 初始化数据库

```bash
# 下载项目源码 “调度数据库初始化SQL脚本” 位置为:
/xxl-job/doc/db/tables_xxl_job.sql
```

1. `- xxl_job_lock：任务调度锁表；`
2. `- xxl_job_group：执行器信息表，维护任务执行器信息；`
3. `- xxl_job_info：调度扩展信息表： 用于保存XXL-JOB调度任务的扩展信息，如任务分组、任务名、机器地址、执行器、执行入参和报警邮件等等；`
4. `- xxl_job_log：调度日志表： 用于保存XXL-JOB任务调度的历史信息，如调度结果、执行结果、调度入参、调度机器和执行器等等；`
5. `- xxl_job_log_report：调度日志报表：用户存储XXL-JOB任务调度日志的报表，调度中心报表功能页面会用到；`
6. `- xxl_job_logglue：任务GLUE日志：用于保存GLUE更新历史，用于支持GLUE的版本回溯功能；`
7. `- xxl_job_registry：执行器注册表，维护在线的执行器和调度中心机器地址信息；`
8. `- xxl_job_user：系统用户表`

### 配置部署“调度中心”

**xxl-job-admin** 统一管理任务调度平台上调度任务，负责触发调度执行，并且提供任务管理平台。

1. 修改配置文件`/xxl-job/xxl-job-admin/src/main/resources/application.properties`,修改数据连接地址

```
# /xxl-job/xxl-job-admin/src/main/resources/application.properties
### xxl-job, datasource
spring.datasource.url=jdbc:mysql://127.0.0.1:3307/xxl_job?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
spring.datasource.username=root
spring.datasource.password=123456
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
```

调度中心访问地址：http://localhost:8080/xxl-job-admin (该地址执行器将会使用到，作为回调地址)

默认登录账号 `admin/123456`, 登录后运行界面如下图所示。

![image-20240201165810413](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/202402011658442.png)

#### 调度中心集群

调度中心支持集群部署，提升调度系统容灾和可用性。

调度中心集群部署时，几点要求和建议：

- DB配置保持一致；
- 集群机器时钟保持一致（单机集群忽视）；
- 建议：推荐通过nginx为调度中心集群做负载均衡，分配域名。调度中心访问、执行器回调配置、调用API服务等操作均通过该域名进行。

```bash
# docker 方式
/**
* 如需自定义 mysql 等配置，可通过 "-e PARAMS" 指定，参数格式 PARAMS="--key=value  --key2=value2" ；
* 配置项参考文件：/xxl-job/xxl-job-admin/src/main/resources/application.properties
* 如需自定义 JVM内存参数 等配置，可通过 "-e JAVA_OPTS" 指定，参数格式 JAVA_OPTS="-Xmx512m" ；
*/
docker run -e PARAMS="--spring.datasource.url=jdbc:mysql://127.0.0.1:3306/xxl_job?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai" -p 8080:8080 -v /tmp:/data/applogs --name xxl-job-admin  -d xuxueli/xxl-job-admin:{指定版本}
```

### 执行器项目

#### 引入依赖

确认pom文件中引入了 “xxl-job-core” 的maven依赖；

```xml
  <!-- xxl-job-core -->
        <dependency>
            <groupId>com.xuxueli</groupId>
            <artifactId>xxl-job-core</artifactId>
            <version>${project.parent.version}</version>
        </dependency>
```

#### 配置文件

```properties
### 调度中心部署根地址 [选填]：如调度中心集群部署存在多个地址则用逗号分隔。执行器将会使用该地址进行"执行器心跳注册"和"任务结果回调"；为空则关闭自动注册；
xxl.job.admin.addresses=http://127.0.0.1:8080/xxl-job-admin 
### 执行器通讯TOKEN [选填]：非空时启用；
xxl.job.accessToken=
### 执行器AppName [选填]：执行器心跳注册分组依据；为空则关闭自动注册
xxl.job.executor.appname=xxl-job-executor-sample
### 执行器注册 [选填]：优先使用该配置作为注册地址，为空时使用内嵌服务 ”IP:PORT“ 作为注册地址。从而更灵活的支持容器类型执行器动态IP和动态映射端口问题。
xxl.job.executor.address=
### 执行器IP [选填]：默认为空表示自动获取IP，多网卡时可手动设置指定IP，该IP不会绑定Host仅作为通讯实用；地址信息用于 "执行器注册" 和 "调度中心请求并触发任务"；
xxl.job.executor.ip=
### 执行器端口号 [选填]：小于等于0则自动获取；默认端口为9999，单机部署多个执行器时，注意要配置不同执行器端口；
xxl.job.executor.port=9999
### 执行器运行日志文件存储磁盘路径 [选填] ：需要对该路径拥有读写权限；为空则使用默认路径；
xxl.job.executor.logpath=/data/applogs/xxl-job/jobhandler
### 执行器日志文件保存天数 [选填] ： 过期日志自动清理, 限制值大于等于3时生效; 否则, 如-1, 关闭自动清理功能；
xxl.job.executor.logretentiondays=30
```

#### 执行器组件，配置文件

```java
package com.xxl.job.executor.core.config;

import com.xxl.job.core.executor.impl.XxlJobSpringExecutor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/**
 * xxl-job config
 *
 * @author xuxueli 2017-04-28
 */
@Configuration
public class XxlJobConfig {
    private Logger logger = LoggerFactory.getLogger(XxlJobConfig.class);

    @Value("${xxl.job.admin.addresses}")
    private String adminAddresses;

    @Value("${xxl.job.accessToken}")
    private String accessToken;

    @Value("${xxl.job.executor.appname}")
    private String appname;

    @Value("${xxl.job.executor.address}")
    private String address;

    @Value("${xxl.job.executor.ip}")
    private String ip;

    @Value("${xxl.job.executor.port}")
    private int port;

    @Value("${xxl.job.executor.logpath}")
    private String logPath;

    @Value("${xxl.job.executor.logretentiondays}")
    private int logRetentionDays;


    @Bean
    public XxlJobSpringExecutor xxlJobExecutor() {
        logger.info(">>>>>>>>>>> xxl-job config init.");
        XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor();
        xxlJobSpringExecutor.setAdminAddresses(adminAddresses);
        xxlJobSpringExecutor.setAppname(appname);
        xxlJobSpringExecutor.setAddress(address);
        xxlJobSpringExecutor.setIp(ip);
        xxlJobSpringExecutor.setPort(port);
        xxlJobSpringExecutor.setAccessToken(accessToken);
        xxlJobSpringExecutor.setLogPath(logPath);
        xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays);

        return xxlJobSpringExecutor;
    }

    /**
     * 针对多网卡、容器内部署等情况，可借助 "spring-cloud-commons" 提供的 "InetUtils" 组件灵活定制注册IP；
     *
     *      1、引入依赖：
     *          <dependency>
     *             <groupId>org.springframework.cloud</groupId>
     *             <artifactId>spring-cloud-commons</artifactId>
     *             <version>${version}</version>
     *         </dependency>
     *
     *      2、配置文件，或者容器启动变量
     *          spring.cloud.inetutils.preferred-networks: 'xxx.xxx.xxx.'
     *
     *      3、获取IP
     *          String ip_ = inetUtils.findFirstNonLoopbackHostInfo().getIpAddress();
     */


}
```



#### 执行器集群

1. 执行器回调地址（xxl.job.admin.addresses）需要保持一致；执行器根据该配置进行执行器自动注册等操作。
2. 同一个执行器集群内AppName（xxl.job.executor.appname）需要保持一致；调度中心根据该配置动态发现不同集群的在线执行器列表。

![image-20240229161548398](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202402291615574.png)

![xxl-job](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061834791.jpg)

#### Job(BEAN)

```java
package com.xxl.job.executor.service.jobhandler;

import com.xxl.job.core.context.XxlJobHelper;
import com.xxl.job.core.handler.annotation.XxlJob;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Component;

import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.DataOutputStream;
import java.io.InputStreamReader;
import java.net.HttpURLConnection;
import java.net.URL;
import java.util.Arrays;
import java.util.concurrent.TimeUnit;

/**
 * XxlJob开发示例（Bean模式）
 *
 * 开发步骤：
 *      1、任务开发：在Spring Bean实例中，开发Job方法；
 *      2、注解配置：为Job方法添加注解 "@XxlJob(value="自定义jobhandler名称", init = "JobHandler初始化方法", destroy = "JobHandler销毁方法")"，注解value值对应的是调度中心新建任务的JobHandler属性的值。
 *      3、执行日志：需要通过 "XxlJobHelper.log" 打印执行日志；
 *      4、任务结果：默认任务结果为 "成功" 状态，不需要主动设置；如有诉求，比如设置任务结果为失败，可以通过 "XxlJobHelper.handleFail/handleSuccess" 自主设置任务结果；
 *
 * @author xuxueli 2019-12-11 21:52:51
 */
@Component
public class SampleXxlJob {
    private static Logger logger = LoggerFactory.getLogger(SampleXxlJob.class);


    /**
     * 1、简单任务示例（Bean模式）
     */
    @XxlJob("demoJobHandler")
    public void demoJobHandler() throws Exception {
        XxlJobHelper.log("XXL-JOB, Hello World.");

        for (int i = 0; i < 5; i++) {
            XxlJobHelper.log("beat at:" + i);
            TimeUnit.SECONDS.sleep(2);
        }
        // default success
    }


    /**
     * 2、分片广播任务
     */
    @XxlJob("shardingJobHandler")
    public void shardingJobHandler() throws Exception {

        // 分片参数
        int shardIndex = XxlJobHelper.getShardIndex();
        int shardTotal = XxlJobHelper.getShardTotal();

        XxlJobHelper.log("分片参数：当前分片序号 = {}, 总分片数 = {}", shardIndex, shardTotal);

        // 业务逻辑
        for (int i = 0; i < shardTotal; i++) {
            if (i == shardIndex) {
                XxlJobHelper.log("第 {} 片, 命中分片开始处理", i);
            } else {
                XxlJobHelper.log("第 {} 片, 忽略", i);
            }
        }

    }


    /**
     * 3、命令行任务
     */
    @XxlJob("commandJobHandler")
    public void commandJobHandler() throws Exception {
        String command = XxlJobHelper.getJobParam();
        int exitValue = -1;

        BufferedReader bufferedReader = null;
        try {
            // command process
            ProcessBuilder processBuilder = new ProcessBuilder();
            processBuilder.command(command);
            processBuilder.redirectErrorStream(true);

            Process process = processBuilder.start();
            //Process process = Runtime.getRuntime().exec(command);

            BufferedInputStream bufferedInputStream = new BufferedInputStream(process.getInputStream());
            bufferedReader = new BufferedReader(new InputStreamReader(bufferedInputStream));

            // command log
            String line;
            while ((line = bufferedReader.readLine()) != null) {
                XxlJobHelper.log(line);
            }

            // command exit
            process.waitFor();
            exitValue = process.exitValue();
        } catch (Exception e) {
            XxlJobHelper.log(e);
        } finally {
            if (bufferedReader != null) {
                bufferedReader.close();
            }
        }

        if (exitValue == 0) {
            // default success
        } else {
            XxlJobHelper.handleFail("command exit value("+exitValue+") is failed");
        }

    }


    /**
     * 4、跨平台Http任务
     *  参数示例：
     *      "url: http://www.baidu.com\n" +
     *      "method: get\n" +
     *      "data: content\n";
     */
    @XxlJob("httpJobHandler")
    public void httpJobHandler() throws Exception {

        // param parse
        String param = XxlJobHelper.getJobParam();
        if (param==null || param.trim().length()==0) {
            XxlJobHelper.log("param["+ param +"] invalid.");

            XxlJobHelper.handleFail();
            return;
        }

        String[] httpParams = param.split("\n");
        String url = null;
        String method = null;
        String data = null;
        for (String httpParam: httpParams) {
            if (httpParam.startsWith("url:")) {
                url = httpParam.substring(httpParam.indexOf("url:") + 4).trim();
            }
            if (httpParam.startsWith("method:")) {
                method = httpParam.substring(httpParam.indexOf("method:") + 7).trim().toUpperCase();
            }
            if (httpParam.startsWith("data:")) {
                data = httpParam.substring(httpParam.indexOf("data:") + 5).trim();
            }
        }

        // param valid
        if (url==null || url.trim().length()==0) {
            XxlJobHelper.log("url["+ url +"] invalid.");

            XxlJobHelper.handleFail();
            return;
        }
        if (method==null || !Arrays.asList("GET", "POST").contains(method)) {
            XxlJobHelper.log("method["+ method +"] invalid.");

            XxlJobHelper.handleFail();
            return;
        }
        boolean isPostMethod = method.equals("POST");

        // request
        HttpURLConnection connection = null;
        BufferedReader bufferedReader = null;
        try {
            // connection
            URL realUrl = new URL(url);
            connection = (HttpURLConnection) realUrl.openConnection();

            // connection setting
            connection.setRequestMethod(method);
            connection.setDoOutput(isPostMethod);
            connection.setDoInput(true);
            connection.setUseCaches(false);
            connection.setReadTimeout(5 * 1000);
            connection.setConnectTimeout(3 * 1000);
            connection.setRequestProperty("connection", "Keep-Alive");
            connection.setRequestProperty("Content-Type", "application/json;charset=UTF-8");
            connection.setRequestProperty("Accept-Charset", "application/json;charset=UTF-8");

            // do connection
            connection.connect();

            // data
            if (isPostMethod && data!=null && data.trim().length()>0) {
                DataOutputStream dataOutputStream = new DataOutputStream(connection.getOutputStream());
                dataOutputStream.write(data.getBytes("UTF-8"));
                dataOutputStream.flush();
                dataOutputStream.close();
            }

            // valid StatusCode
            int statusCode = connection.getResponseCode();
            if (statusCode != 200) {
                throw new RuntimeException("Http Request StatusCode(" + statusCode + ") Invalid.");
            }

            // result
            bufferedReader = new BufferedReader(new InputStreamReader(connection.getInputStream(), "UTF-8"));
            StringBuilder result = new StringBuilder();
            String line;
            while ((line = bufferedReader.readLine()) != null) {
                result.append(line);
            }
            String responseMsg = result.toString();

            XxlJobHelper.log(responseMsg);

            return;
        } catch (Exception e) {
            XxlJobHelper.log(e);

            XxlJobHelper.handleFail();
            return;
        } finally {
            try {
                if (bufferedReader != null) {
                    bufferedReader.close();
                }
                if (connection != null) {
                    connection.disconnect();
                }
            } catch (Exception e2) {
                XxlJobHelper.log(e2);
            }
        }

    }

    /**
     * 5、生命周期任务示例：任务初始化与销毁时，支持自定义相关逻辑；
     */
    @XxlJob(value = "demoJobHandler2", init = "init", destroy = "destroy")
    public void demoJobHandler2() throws Exception {
        XxlJobHelper.log("XXL-JOB, Hello World.");
    }
    public void init(){
        logger.info("init");
    }
    public void destroy(){
        logger.info("destroy");
    }


}

```

### 架构

#### 设计思想

将调度行为抽象形成“调度中心”公共平台，而平台自身并不承担业务逻辑，“调度中心”负责发起调度请求。

将任务抽象成分散的JobHandler，交由“执行器”统一管理，“执行器”负责接收调度请求并执行对应的JobHandler中业务逻辑。

因此，“调度”和“任务”两部分可以相互解耦，提高系统整体稳定性和扩展性；

#### 系统组成

- **调度模块（调度中心）**：
  负责管理调度信息，按照调度配置发出调度请求，自身不承担业务代码。调度系统与任务解耦，提高了系统可用性和稳定性，同时调度系统性能不再受限于任务模块；
  支持可视化、简单且动态的管理调度信息，包括任务新建，更新，删除，GLUE开发和任务报警等，所有上述操作都会实时生效，同时支持监控调度结果以及执行日志，支持执行器Failover。
- **执行模块（执行器）**：
  负责接收调度请求并执行任务逻辑。任务模块专注于任务的执行等操作，开发和维护更加简单和高效；
  接收“调度中心”的执行请求、终止请求和日志请求等。

#### 架构图

![img_Qohm](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403041329391.png)



#### 自研调度模块

XXL-JOB最终选择自研调度组件（早期调度组件基于Quartz）；一方面是为了精简系统降低冗余依赖，另一方面是为了提供系统的可控度与稳定性；

XXL-JOB中“调度模块”和“任务模块”完全解耦，调度模块进行任务调度时，将会解析不同的任务参数发起远程调用，调用各自的远程执行器服务。这种调用模型类似RPC调用，调度中心提供调用代理的功能，而执行器提供远程服务的功能。

![image-20240306175447044](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061754115.png)



当调度中心启动后，会启动以下两个线程：

1. schedulerThread `scheudlerThread`主要做如下两件事情：

- 从数据中心（db），也就是 

  ```sql
  xxl_job_info
  ```

  表中扫描出符合 `条件 1`的任务， 

  `条件1`

   限制如下：

  - 任务执行时间 **小于**（当前时间 + 5 s）
  - 限制扫描个数， 这个值是动态的，会根据后面的提到的 **快慢线程池** 中线程数量有关系。

```bash
count = treadpool-size * trigger-qps  (each trigger cost 50ms, qps = 1000/50 = 20) 

treadpool-size = (getTriggerPoolFastMax() + getTriggerPoolSlowMax()) * 20
```

![image-20240306175757971](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061757022.png)

2. ringThread

`ringThread`的作用就是不断从 **容器** 中读取 **当前时间点需要执行** 的任务， 读取出来的任务会交给一个叫 **快慢线程池** 的东西去将任务传递给**调度器**去执行。

快慢线程池

```java
 public void start(){
        fastTriggerPool = new ThreadPoolExecutor(
                10,
                XxlJobAdminConfig.getAdminConfig().getTriggerPoolFastMax(),
                60L,
                TimeUnit.SECONDS,
                new LinkedBlockingQueue<Runnable>(1000),
                new ThreadFactory() {
                    @Override
                    public Thread newThread(Runnable r) {
                        return new Thread(r, "xxl-job, admin JobTriggerPoolHelper-fastTriggerPool-" + r.hashCode());
                    }
                });

        slowTriggerPool = new ThreadPoolExecutor(
                10,
                XxlJobAdminConfig.getAdminConfig().getTriggerPoolSlowMax(),
                60L,
                TimeUnit.SECONDS,
                new LinkedBlockingQueue<Runnable>(2000),
                new ThreadFactory() {
                    @Override
                    public Thread newThread(Runnable r) {
                        return new Thread(r, "xxl-job, admin JobTriggerPoolHelper-slowTriggerPool-" + r.hashCode());
                    }
                });
    }
```





#### 调度中心HA（集群）

基于数据库的集群方案，数据库选用Mysql；集群分布式并发环境中进行定时任务调度时，会在各个节点会上报任务，存到数据库中，执行时会从数据库中取出触发器来执行，如果触发器的名称和执行时间相同，则只有一个节点去执行此任务。

#### 调度线程池

调度采用线程池方式实现，避免单线程因阻塞而引起任务调度延迟。

#### 并行调度

XXL-JOB调度模块默认采用并行机制，在多线程调度的情况下，调度模块被阻塞的几率很低，大大提高了调度系统的承载量。

XXL-JOB的不同任务之间并行调度、并行执行。 XXL-JOB的单个任务，针对多个执行器是并行运行的，针对单个执行器是串行执行的。同时支持任务终止。

#### 过期处理策略

任务调度错过触发时间时的处理策略：

- 可能原因：服务重启；调度线程被阻塞，线程被耗尽；上次调度持续阻塞，下次调度被错过；
- 处理策略：
  - 过期超5s：本次忽略，当前时间开始计算下次触发时间
  - 过期5s内：立即触发一次，当前时间开始计算下次触发时间

#### 任务HA（Failover）

执行器如若集群部署，调度中心将会感知到在线的所有执行器，如“127.0.0.1:9997, 127.0.0.1:9998, 127.0.0.1:9999”。

当任务”路由策略”选择”故障转移(FAILOVER)”时，当调度中心每次发起调度请求时，会按照顺序对执行器发出心跳检测请求，第一个检测为存活状态的执行器将会被选定并发送调度请求。

#### 均衡调度    

调度中心在集群部署时会自动进行任务平均分配，触发组件每次获取与线程池数量（调度中心支持自定义调度线程池大小）相关数量的任务，避免大量任务集中在单个调度中心集群节点

#### 故障转移 & 失败重试

一次完整任务流程包括”调度（调度中心） + 执行（执行器）”两个阶段。

- “故障转移”发生在调度阶段，在执行器集群部署时，如果某一台执行器发生故障，该策略支持自动进行Failover切换到一台正常的执行器机器并且完成调度请求流程。
- “失败重试”发生在”调度 + 执行”两个阶段，支持通过自定义任务失败重试次数，当任务失败时将会按照预设的失败重试次数主动进行重试；

#### 执行器灰度上线

调度中心与业务解耦，只需部署一次后常年不需要维护。但是，执行器中托管运行着业务作业，作业上线和变更需要重启执行器，尤其是Bean模式任务。
执行器重启可能会中断运行中的任务。但是，XXL-JOB得益于自建执行器与自建注册中心，可以通过灰度上线的方式，避免因重启导致的任务中断的问题。

步骤如下：

- 1、执行器改为手动注册，下线一半机器列表（A组），线上运行另一半机器列表（B组）；
- 2、等待A组机器任务运行结束并编译上线；执行器注册地址替换为A组；
- 3、等待B组机器任务运行结束并编译上线；执行器注册地址替换为A组+B组；
  操作结束；

#### 避免任务重复执行

调度密集或者耗时任务可能会导致任务阻塞，集群情况下调度组件小概率情况下会重复触发；
针对上述情况，可以通过结合 “单机路由策略（如：第一台、一致性哈希）” + “阻塞策略（如：单机串行、丢弃后续调度）” 来规避，最终避免任务重复执行。

---

## PowerJob



### 部署调度中心（docker版）

```dockerfile
docker run -d \
 			 --restart=always \
       --name powerjob-server \
       -p 7700:7700 -p 10086:10086 -p 10010:10010 \
       -e TZ="Asia/Shanghai" \
       -e JVMOPTIONS="" \
       -e PARAMS="--spring.profiles.active=product --spring.datasource.core.jdbc-url=jdbc:mysql://192.168.70.58:3307/powerjob-product?useUnicode=true&characterEncoding=UTF-8 --spring.datasource.core.username=root --spring.datasource.core.password=123456 --spring.data.mongodb.uri=mongodb://192.168.70.58:27017/powerjob-product" \
       -v ~/Space/powerjob-server:/root/powerjob/server -v ~/.m2:/root/.m2 \
       powerjob/powerjob-server:latest
```

### 注册应用

![image-20240228163908967](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/202402281639216.png)

### 登录

Username:应用名称

password：应用密码

```bash
# 测试帐号密码
cong/123456
```

![image-20240228164121786](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/202402281641820.png)

### **[官方处理器](https://www.yuque.com/powerjob/guidence/official_processor)**

**PowerJob 支持 Python、Shell、HTTP、SQL 等众多通用任务的处理，只需要引入依赖，在控制台配置好相关参数即可**

```xml
  <!-- 官方处理器 -->
        <dependency>
            <groupId>tech.powerjob</groupId>
            <artifactId>powerjob-official-processors</artifactId>
            <version>${powerjob.version}</version>
        </dependency>
```

```bash
# Shell 处理器
全限定类名 tech.powerjob.official.processors.impl.script.ShellProcessor
# Python 处理器
全限定类名 tech.powerjob.official.processors.impl.script.PythonProcessor

# HTTP 处理器
全限定类名 tech.powerjob.official.processors.impl.HttpProcessor
任务参数（JSON）
● method【必填字段】：GET / POST / DELETE / PUT
● url【必填字段】：请求地址
● timeout【可选字段】：超时时间，单位为秒
● mediaType【可选字段】：使用非 GET 请求时，需要传递的数据类型，如 `application/json`
● body【可选字段】：使用非 GET 请求时的 body 内容，后端使用 String 接收，如果为 JSON 请注意转义
● headers【可选字段】：请求头，后端使用 Map<String, String> 接收
# 其他更多参见官方文档
```





### 处理器（Processor）开发

- 引入依赖，最新稳定版本为`4.3.8`，对应的spring boot版本为`2.7.18`

````xml
       <!-- https://mvnrepository.com/artifact/tech.powerjob/powerjob-worker-spring-boot-starter -->
        <dependency>
            <groupId>tech.powerjob</groupId>
            <artifactId>powerjob-worker-spring-boot-starter</artifactId>
            <version>${powerjob.version}</version>
        </dependency>
      
````

```properties
server.port=11714
# akka 工作端口，可选，默认 27777
powerjob.worker.port=27777
# 接入应用名称，用于分组隔离，推荐填写 本 Java 项目名称
powerjob.worker.app-name=cong
# 调度服务器地址，IP:Port 或 域名，多值逗号分隔
powerjob.worker.server-address=192.168.70.58:7700
# 通讯协议，4.3.0 开始支持 HTTP 和 AKKA 两种协议，官方推荐使用 HTTP 协议（注意 server 和 worker 都要开放相应端口）
powerjob.worker.protocol=http
# 持久化方式，可选，默认 disk
powerjob.worker.store-strategy=disk
# 任务返回结果信息的最大长度，超过这个长度的信息会被截断，默认值 8192
powerjob.worker.max-result-length=4096
# 单个任务追加的工作流上下文最大长度，超过这个长度的会被直接丢弃，默认值 8192
powerjob.worker.max-appended-wf-context-length=4096
# 同时运行的轻量级任务数量上限
powerjob.worker.max-lightweight-task-num=1024
# 同时运行的重量级任务数量上限
powerjob.worker.max-heavy-task-num=64
```

- 启动服务 观察日志

```bash
024-02-28 17:31:36.017  INFO 20958 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 11714 (http) with context path ''
2024-02-28 17:31:36.022  INFO 20958 --- [           main] c.c.p.PowerjobWorkerApplication          : Started PowerjobWorkerApplication in 7.763 seconds (JVM running for 8.083)
2024-02-28 17:31:46.013  INFO 20958 --- [b-worker-core-0] t.p.w.background.WorkerHealthReporter    : [WorkerHealthReporter] report health status,appId:1,appName:cong,isOverload:false,maxLightweightTaskNum:1024,currentLightweightTaskNum:0,maxHeavyweightTaskNum:64,currentHeavyweightTaskNum:0
2024-02-28 17:31:56.021  INFO 20958 --- [b-worker-core-0] t.p.w.background.WorkerHealthReporter    : [WorkerHealthReporter] report health status,appId:1,appName:cong,isOverload:false,maxLightweightTaskNum:1024,currentLightweightTaskNum:0,maxHeavyweightTaskNum:64,currentHeavyweightTaskNum:0
2024-02-28 17:32:06.031  INFO 20958 --- [b-worker-core-0] t.p.w.background.WorkerHealthReporter    : [WorkerHealthReporter] report health status,appId:1,appName:cong,isOverload:false,maxLightweightTaskNum:1024,currentLightweightTaskNum:0,maxHeavyweightTaskNum:64,currentHeavyweightTaskNum:0
```

- 管理界面

  ![image-20240228173324858](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/202402281733946.png)

  - 配置任务

  [演示地址](http://192.168.70.58:7700/)

- 单机处理器：BasicProcessor

```java
// 支持 SpringBean 的形式
//单机执行的策略下，server 会在所有可用 worker 中选取健康度最佳的机器进行执行。
@Component
public class BasicProcessorDemo implements BasicProcessor {

    @Resource
    private MysteryService mysteryService;

    @Override
    public ProcessResult process(TaskContext context) throws Exception {

        // 在线日志功能，可以直接在控制台查看任务日志，非常便捷
        OmsLogger omsLogger = context.getOmsLogger();
        omsLogger.info("BasicProcessorDemo start to process, current JobParams is {}.", context.getJobParams());
        
        // TaskContext为任务的上下文信息，包含了在控制台录入的任务元数据，常用字段为
        // jobParams（任务参数，在控制台录入），instanceParams（任务实例参数，通过 OpenAPI 触发的任务实例才可能存在该参数）

        // 进行实际处理...
        mysteryService.hasaki();

        // 返回结果，该结果会被持久化到数据库，在前端页面直接查看，极为方便
        return new ProcessResult(true, "result is xxx");
    }
}
```

- 广播处理器：BroadcastProcessor

```java
//广播执行的策略下，所有机器都会被调度执行该任务。为了便于资源的准备和释放，
//广播处理器在BasicProcessor 的基础上额外增加了 preProcess 和 postProcess 方法，分别在整个集群开始之前/结束之后选一台机器执行相关方法。
@Component
public class BroadcastProcessorDemo implements BroadcastProcessor {

    @Override
    public ProcessResult preProcess(TaskContext taskContext) throws Exception {
        // 预执行，会在所有 worker 执行 process 方法前调用
        return new ProcessResult(true, "init success");
    }

    @Override
    public ProcessResult process(TaskContext context) throws Exception {
        // 撰写整个worker集群都会执行的代码逻辑
        return new ProcessResult(true, "release resource success");
    }

    @Override
    public ProcessResult postProcess(TaskContext taskContext, List<TaskResult> taskResults) throws Exception {

        // taskResults 存储了所有worker执行的结果（包括preProcess）

        // 收尾，会在所有 worker 执行完毕 process 方法后调用，该结果将作为最终的执行结果
        return new ProcessResult(true, "process success");
    }
}
```

### 架构

![image-20240305092119644](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403050921697.png)

#### Server 启动流程分析

![2024030117428701](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061656796.png)

#### 集群

![2024030117428701](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061658564.png)

### 无锁化调度的实现

#### 经典调度原理

```sql
-- 师承老祖宗 quartz：
select * from job_info where next_trigger_time < now() + 15s
```

![img](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403050949302.png)

#### 重复调度

为了解决重复调度，当前开源调度框架的解决方案：锁

- select for update
- 分布式锁

> 本质上只有一台 server 在真正提供服务！集群部署只获得了 高可用能力，并没有获取性能的提升，不具备水平扩展性

#### 无锁化调度

- 引入分组依据  AppName，以应用集群作为 server 调度的单位。 

- 每一个 worker 集群在运行时只会连接到某一台 server。 

- 每一个 server 实例只会调度当前与自己保持心跳的 worker 关联的 AppName 下的所有任务。

```sql
select * from job_info where app_name = xxx and  next_trigger_time < now() + 15s
```

![img](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061700771.png)

> worker 因为没办法获取 server 的准确状态，所以不能由 worker 来决定连接哪一台 server。因此，worker 需要做的，只是**服务发现**。即定时使用 HTTP 请求任意一台 server，请求获取当前该分组（appName）对应的 server。
>
> 而 server 收到来自 worker 的服务发现请求后，其实就是进行了一场小型的分布式选主：server 依赖的数据库中存在着 server_info 表，其中记录了每一个分组（appName）所对应的 server 信息。如果该 server 发现表中存在记录，那就说明该 worker 集群中已经有别的 worker 事先请求 server 进行选举，那么此时只需要发送 PING 请求检测该 server 是否存活。如果发现该 server 存活，那么直接返回该 server 的信息作为该分组的 server。否则就完成篡位，将自己的信息写入数据库表中，成为该分组的 server。
>
> 细心的小伙伴可能又要问了？发送 PING 请求检测该 server 是否存活，不还是有和刚才一样的问题吗？请求不同，发送方和接收方都有可能出问题，凭什么认为是原先的 server 挂了呢？
>
> 确实，在这个方案下，依旧没办法解决 server 到底挂没挂这个堪比“真假美猴王”的玄学问题。但是，这还重要吗？我们的目标是某个分组下所有的 worker 都连接到同一台 server，因此，即便产生那种误打误撞篡位的情况，在服务发现机制的加持下，整个集群最终还是会连接到同一台 server，完美实现我们的需求。

### 任务配置

![image.png](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403051359180.png)

#### 可靠调度——WAL

>  **WAL（Write-Ahead Logging，预写式日志）**，这是主流关系型数据库（MS SQLServer、MySQL、Oracle）用来确保了事务原子性和持久性的关键技术。WAL 的核心思想是：**在数据写入到数据库之前，先写入到日志中。**这样，在硬盘数据不损坏的情况下，预写式日志允许存储系统在崩溃后能够在日志的指导下恢复到崩溃前的状态，避免数据丢失。
>
> PowerJob 为了实现任务的可靠调度，也借鉴了该思想。每一个任务被调度执行时，系统都会为其生成一条记录，这条记录包含了该任务实例（任务的一次运行叫任务实例）的预期调度时间。之后，PowerJob 会首先将该记录持久化到数据库中，只有持久化成功后，该任务才会被正式推入时间轮进行调度。
>
> 一旦这一台 server 宕机，任务没有被准时执行。其他 server 就能根据已经写入数据库中的任务实例记录将其恢复，做到可靠调度～
>
> 也就是说，只要你的系统中还有一台 powerjob-server 活着，就不会有缺失调度的情况。

#### 秒级任务

> 每一个秒级任务，都会直接被投递到集群中的某一台 powerjob-worker 上，由 powerjob-worker 全权负责执行。而 powerjob-server 此时只需要负责故障恢复即可。
>
> 这样一来，server 的压力进一步减轻，同时，由于秒级任务的调度与执行全部落在了 worker 身上，调度的精度也会上升（至少能省下通讯的网络延迟）

---

## Apache DolphinScheduler

主要面向大数据处理领域，其核心需求是按照规定流程（ DAG ）跑一堆脚本去完成一些数据任务。

[试用地址](http://192.168.70.58:12345/dolphinscheduler/ui/home)

![image-20240306165251004](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061652048.png)



#### 容错设计

服务容错设计依赖于ZooKeeper的Watcher机制，实现原理如图

其中Master监控其他Master和Worker的目录，如果监听到remove事件，则会根据具体的业务逻辑进行流程实例容错或者任务实例容错。

- Master容错流程

![容错流程](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403051716004.jpg)

容错范围：从host的维度来看，Master的容错范围包括：自身host+注册中心上不存在的节点host，容错的整个过程会加锁；

容错内容：Master的容错内容包括：容错工作流实例和任务实例，在容错前会比较实例的开始时间和服务节点的启动时间，在服务启动时间之后的则跳过容错；

容错后处理：ZooKeeper Master容错完成之后则重新由DolphinScheduler中Scheduler线程调度，遍历 DAG 找到”正在运行”和“提交成功”的任务，对”正在运行”的任务监控其任务实例的状态，对”提交成功”的任务需要判断Task Queue中是否已经存在，如果存在则同样监控任务实例的状态，如果不存在则重新提交任务实例。

Worker容错流程

![容错流程](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403051724743.jpg)





容错范围：从工作流实例的维度看，每个Worker只负责容错自己的工作流实例；只有在`handleDeadServer`时会加锁；

容错内容：当发送Worker节点的remove事件时，Master只容错任务实例，在容错前会比较实例的开始时间和服务节点的启动时间，在服务启动时间之后的则跳过容错；

容错后处理：Master Scheduler线程一旦发现任务实例为” 需要容错”状态，则接管任务并进行重新提交。

注意：由于” 网络抖动”可能会使得节点短时间内失去和ZooKeeper的心跳，从而发生节点的remove事件。对于这种情况，我们使用最简单的方式，那就是节点一旦和ZooKeeper发生超时连接，则直接将Master或Worker服务停掉。



#### 负载均衡

- 预热

考虑到 JIT 优化，会让 worker 在启动后低功率的运行一段时间，使其逐渐达到最佳状态，这段过程称之为预热。

- #### 随机（加权）

在符合的 worker 中随机选取一台（权重会影响他的比重）

- #### 平滑轮询（加权）

加权轮询算法一个明显的缺陷。即在某些特殊的权重下，加权轮询调度会生成不均匀的实例序列，这种不平滑的负载可能会使某些实例出现瞬时高负载的现象，导致系统存在宕机的风险。为了解决这个调度缺陷，我们提供了平滑加权轮询算法。

每台 worker 都有两个权重，即 weight（预热完成后保持不变），current_weight（动态变化），每次路由。都会遍历所有的 worker，使其 current_weight+weight，同时累加所有 worker 的 weight，计为 total_weight，然后挑选 current_weight 最大的作为本次执行任务的 worker，与此同时，将这台 worker 的 current_weight-total_weight。

- #### 线性加权(默认算法)

该算法每隔一段时间会向注册中心上报自己的负载信息。我们主要根据两个信息来进行判断

- load 平均值（默认是 CPU 核数 *2）
- 可用物理内存（默认是 0.3，单位是 G）

如果两者任何一个低于配置项，那么这台 worker 将不参与负载。（即不分配流量）

可以在 worker.properties 修改下面的属性来自定义配置

- worker.max.cpu.load.avg=-1 (worker最大cpu load均值，只有高于系统cpu load均值时，worker服务才能被派发任务. 默认值为-1: cpu cores * 2)
- worker.reserved.memory=0.3 (worker预留内存，只有低于系统可用内存时，worker服务才能被派发任务，单位为百分比)

___

## Elastic-Job

- 支持分布式部署；不同节点上执行的是不一样的任务(代码是同一套)；对于一个大任务，可以用分片策略，让他在多节点上执行；
- 能够保证高可用；
- 利用zk实现分布式环境管理；
- 水平扩展（核心）

### **快速开始**

```xml
<dependency>
 <groupId>org.apache.shardingsphere.elasticjob</groupId>
 <artifactId>elasticjob-bootstrap</artifactId>
 <version>${latest.release.version}</version>
</dependency>
```



```java
public class MyJob implements SimpleJob {

  @Override
  public void execute(ShardingContext context) {
    switch (context.getShardingItem()) {
      case 0: 
        // do something by sharding item 0
        break;
      case 1: 
        // do something by sharding item 1
        break;
      case 2: 
        // do something by sharding item 2
        break;
        // case n: ...
    }
  }
}

//作业调度
public class MyJobDemo {
  
  public static void main(String[] args) {
    new ScheduleJobBootstrap(createRegistryCenter(), new MyJob(), createJobConfiguration()).schedule();
  }
  
  private static CoordinatorRegistryCenter createRegistryCenter() {
    CoordinatorRegistryCenter regCenter = new ZookeeperRegistryCenter(new ZookeeperConfiguration("zk_host:2181", "my-job"));
    regCenter.init();
    return regCenter;
  }
  
  private static JobConfiguration createJobConfiguration() {
    // 创建作业配置
   
    JobConfiguration jobConfig = JobConfiguration.newBuilder("MyJob", 3).cron("0/5 * * * * ?").build();
     // ...
  }
}


```





### **分片**

ElasticJob 中任务分片项的概念，使得任务可以在分布式的环境下运行，每台任务服务器只运行分配给该服务器的分片。 随着服务器的增加或宕机，ElasticJob 会近乎实时的感知服务器数量的变更，从而重新为分布式的任务服务器分配更加合理的任务分片项，使得任务可以随着资源的增加而提升效率。

任务的分布式执行，需要将一个任务拆分为多个独立的任务项，然后由分布式的服务器分别执行某一个或几个分片项。

![image-20240306105742699](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061057770.png)





当新增加作业服务器时，ElasticJob 会通过注册中心的临时节点的变化感知到新服务器的存在，并在下次任务调度的时候重新分片，新的服 务器会承载一部分作业分片

### **高可用**

当作业服务器在运行中宕机时，注册中心同样会通过临时节点感知，并将在下次运行时将分片转移至仍存活的服务器，以达到作业高可用的效果。 本次由于服务器宕机而未执行完的作业，则可以通过失效转移的方式继续执行。

将分片总数设置为 1，并使用多于 1 台的服务器执行作业，作业将会以 1 主 n 从的方式执行。 一旦执行作业的服务器宕机，等待执行的服务器将会在下次作业启动时替补执行。开启失效转移功能效果更好，如果本次作业在执行过程中宕机，备机会立即替补执行。

![image-20240306110506655](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061105712.png)

#### 实现原理

 ElasticJob 并无作业调度中心节点，而是基于部署作业框架的程序在到达相应时间点时各自触发调度。注 册中心仅用于作业注册和监控信息存储。而主作业节点仅用于处理分片和清理等功能。 

弹性分布式实现 

• 第一台服务器上线触发主服务器选举。主服务器一旦下线，则重新触发选举，选举过程中阻塞，只 有主服务器选举完成，才会执行其他任务。 

• 某作业服务器上线时会自动将服务器信息注册到注册中心，下线时会自动更新服务器状态。 

• 主节点选举，服务器上下线，分片总数变更均更新重新分片标记。

 • 定时任务触发时，如需重新分片，则通过主服务器分片，分片过程中阻塞，分片结束后才可执行任 务。如分片过程中主服务器下线，则先选举主服务器，再分片。 

• 通过上一项说明可知，为了维持作业运行时的稳定性，运行过程中只会标记分片状态，不会重新分 片。分片仅可能发生在下次任务触发前。 • 每次分片都会按服务器 IP 排序，保证分片结果不会产生较大波动。 

• 实现失效转移功能，在某台服务器执行完毕后主动抓取未分配的分片，并且在某台服务器下线后主 动寻找可用的服务器执行任务。

### [**错过任务重执行**](https://shardingsphere.apache.org/elasticjob/current/cn/features/misfire/)

ElasticJob 不允许作业在同一时间内叠加执行。 当作业的执行时长超过其运行间隔，错过任务重执行能够保证作业在完成上次的任务后继续执行逾期的作业。



#### 作业启动

![作业启动](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061115367.jpg)

#### 作业执行

![作业执行](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061116217.jpg)

#### ej失效转移

ElasticJob 不会在本次执行过程中进行重新分片，而是等待下次调度之前才开启重新分片流程。 当作业执行过程中服务器宕机，失效转移允许将该次未完成的任务在另一作业节点上补偿执行。



#### 线程池策略

线程池策略，用于执行作业的线程池创建。

| *SPI 名称*                        | *详细说明*               |
| :-------------------------------- | :----------------------- |
| JobExecutorThreadPoolSizeProvider | 作业执行线程数量提供策略 |

| *已知实现类*                                  | *详细说明*                           |
| :-------------------------------------------- | :----------------------------------- |
| CPUUsageJobExecutorThreadPoolSizeProvider     | 根据 CPU 核数 * 2 创建作业处理线程池 |
| SingleThreadJobExecutorThreadPoolSizeProvider | 使用单线程处理作业                   |

#### 设置合理的超时时间

```java
// http 类型任务
 JobConfiguration jobConfiguration = JobConfiguration.newBuilder("javaHttpJob", 1)
                .setProperty(HttpJobProperties.URI_KEY, "http://192.168.70.58:8080/job/test")
                .setProperty(HttpJobProperties.METHOD_KEY, "GET")
                .setProperty(HttpJobProperties.DATA_KEY, "source=ejob")
   							//超时时间
                .setProperty(HttpJobProperties.READ_TIMEOUT_KEY, "120000")
                .setProperty(HttpJobProperties.CONNECT_TIMEOUT_KEY, "60000")
                .cron("0/15 * * * * ? *")
                .shardingItemParameters("0=test0,1=test1, 2=test2, 3=test3")
                .failover(true)
                .misfire(true)
                .overwrite(true)
                .build();

        ScheduleJobBootstrap scheduleJobBootstrap = new ScheduleJobBootstrap(zookeeperRegistryCenter, "HTTP",
                jobConfiguration);
        scheduleJobBootstrap.schedule();
```



### 运维平台配置

```bash
https://dlcdn.apache.org/shardingsphere/elasticjob-ui-3.0.2/apache-shardingsphere-elasticjob-3.0.2-lite-ui-bin.tar.gz
```

#### 启动

```
./shart.sh
```

浏览器打开 http://localhost:8088/，用户名/密码：root/root

连接zookeeper

![image-20240301113136175](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061624510.png)

### 作业开发

ElasticJob 目前提供 Simple、Dataflow 这两种基于 class 的作业类型，并提供 Script、HTTP 这两种基于 type 的作业类型，用户可通过实现 SPI 接口自行扩展作业类型。

- http

```java

public class HttpJobMain {
    
    public static void main(String[] args) {
        
        new ScheduleJobBootstrap(regCenter, "HTTP", JobConfiguration.newBuilder("javaHttpJob", 1)
                .setProperty(HttpJobProperties.URI_KEY, "http://xxx.com/execute")
                .setProperty(HttpJobProperties.METHOD_KEY, "POST")
                .setProperty(HttpJobProperties.DATA_KEY, "source=ejob")
                .cron("0/5 * * * * ?").shardingItemParameters("0=Beijing").build()).schedule();
    }
}

```

#### zookeeper数据

![image-20240306110814324](https://test-fsservice.oss-cn-shanghai.aliyuncs.com/fs/test/2024/202403061108369.png)

---

## 对比

| 特性     | quartz                                                 | elastic-job-lite                                             | xxl-job                                                      | PowerJob                  |
| :------- | :----------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- | ------------------------- |
| 依赖     | MySQL、jdk                                             | jdk、zookeeper                                               | mysql、jdk                                                   | MySQL、JDK、MongoDB(可选) |
| 高可用   | 多节点部署，通过竞争数据库锁来保证只有一个节点执行任务 | 通过zookeeper的注册与发现，可以动态的添加服务器              | 基于竞争数据库锁保证只有一个节点执行任务，支持水平扩容。可以手动增加定时任务，启动和暂停任务，有监控 |                           |
| 任务分片 | ×                                                      | √                                                            | √                                                            | MapReduce动态分片         |
| 管理界面 | ×                                                      | √                                                            | √                                                            | √                         |
| 难易程度 | 简单                                                   | 简单                                                         | 简单                                                         | 简单                      |
| 高级功能 | -                                                      | 弹性扩容，多种作业模式，失效转移，运行状态收集，多线程处理数据，幂等性，容错处理，spring命名空间支持 | 弹性扩容，分片广播，故障转移，Rolling实时日志，GLUE（支持在线编辑代码，免发布）,任务进度监控，任务依赖，数据加密，邮件报警，运行报表，国际化 | DAG工作流                 |
| 版本更新 | 半年没更新                                             | 2023-10-14 3.0.4                                             | 最近有更新                                                   | 最近有更新                |



## 任务分配细节调研比较

| 项目\方案                                                    | qz                                                           | Xxl-job                                             | p-j                              | e-j                             | d-s                     |
| ------------------------------------------------------------ | :----------------------------------------------------------- | --------------------------------------------------- | -------------------------------- | ------------------------------- | ----------------------- |
| 任务分配的方式如何保证公平                                   | [集群任务调度的原理](#集群任务调度的原理)                    | [均衡调度](#均衡调度)                               | [任务配置](#任务配置)            | [分片](#分片）)                 | [负载均衡](#负载均衡)   |
| 每个执行器的容量上限                                         | [同时执行的最大任务数](#同时执行的最大任务数)                | 线程池                                              | 线程池                           | [线程池策略](#线程池策略)       | 负载状况决定,和硬件相关 |
| 当pod重启时被分配的任务会怎么处理（如果有任务正在执行时会怎么样） | [处理集群环境下的任务冲突和容错](#处理集群环境下的任务冲突和容错) | [过期处理策略](#过期处理策略),[灰度上线](#灰度上线) | [可靠调度WAL](#可靠调度——WAL)    | [高可用](#高可用)               | [容错设计](#容错设计)   |
| 长事务的任务执行会不会有问题                                 | [Quartz处理耗时较长的任务](#Quartz处理耗时较长的任务)        | [避免任务重复执行](#避免任务重复执行)               | 设置合适的间隔时间和运行时间限制 | [超时时间](#设置合理的超时时间) | 设置超时时间            |



## 参考资料

- [PowerJob](https://www.yuque.com/powerjob)
- [elastic-job](https://shardingsphere.apache.org/elasticjob/index_zh.html)
- [xxl-job](https://www.xuxueli.com/xxl-job/)
- [DolphinScheduler](https://dolphinscheduler.apache.org/zh-cn/docs/3.1.5/guide/start/docker)
- [PowerJob源码分析-分组隔离设计](https://zhuanlan.zhihu.com/p/163487886)
